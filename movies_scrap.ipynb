{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LEGEND:\n",
    "r_ = request\n",
    "s_ = soup\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_movies(page, progress_counter, release_country = 'USA'):\n",
    "    data = {}\n",
    "    \n",
    "    #for i in range(page_counter[0], page_counter[1], 50): #Looping over pages on imdb\n",
    "        \n",
    "       \n",
    "    # 1. Get request model of imdb movies and convert it to str with .txt\n",
    "    r_movies = requests.get(f'https://www.imdb.com/search/title/?title_type=feature,documentary&release_date=1980-01-01,&sort=num_votes,desc&start={progress_counter}').text\n",
    "    # 2. Use soup to parse html object\n",
    "    s_movies = BeautifulSoup(r_movies, 'html.parser')\n",
    "\n",
    "    # Looking for info on main list\n",
    "    # Finding div with all movie info\n",
    "    for index, div in enumerate(s_movies.find_all('div', class_='lister-item-content')):\n",
    "\n",
    "        # Progress counter\n",
    "        print('Page', page)\n",
    "        print('Movies left:', 50-len(data)) \n",
    "        info = {'title_id': None, 'year': None, 'release_date': None, 'rating':None, 'votes_number': None, 'certificate':None, 'runtime':None, 'genres':None, 'director':None}\n",
    "\n",
    "        ###### Scrapping title ID and title: ######\n",
    "        for h3 in div.find_all('h3'):\n",
    "            #titles.append(h3.a.text)\n",
    "            title_id = (h3.a.get('href'))\n",
    "            info['title_id'] = title_id\n",
    "            movie_url = 'https://www.imdb.com'+title_id\n",
    "\n",
    "            ###### Scrapping year ######\n",
    "            for span in h3.find_all('span', class_='lister-item-year'):\n",
    "                year_str = span.text\n",
    "                year = re.search('\\d\\d\\d\\d',year_str).group()\n",
    "                info['year'] = int(year)\n",
    "\n",
    "        ###### Scrapping certificate, runtime, genres ######\n",
    "        for p in div.find_all('p'):\n",
    "\n",
    "            ###### Certificates                \n",
    "            for cert in p.find_all('span', class_='certificate'):\n",
    "                info['certificate'] = cert.text\n",
    "\n",
    "            ###### Runtime\n",
    "            for runtime in p.find_all('span', class_='runtime'):\n",
    "                info['runtime'] = int(re.search(r'\\d*', runtime.text).group())\n",
    "\n",
    "            ###### Genres\n",
    "            for genre in p.find_all('span', class_='genre'):\n",
    "                split = re.split(',', genre.text)\n",
    "                sub = [re.sub('(\\n)|( *)', '', x) for x in split]\n",
    "                info['genres'] = sub\n",
    "\n",
    "        ###### Scrapping ratings ######\n",
    "        for rating in div.find_all('div', class_='ratings-bar'):\n",
    "            ratings.append(rating.strong.text)\n",
    "            info['rating'] = rating.strong.text\n",
    "\n",
    "        ###### Scrapping vote numbers ######\n",
    "        for p in div.find_all('p'):\n",
    "            container = []\n",
    "            for votes in p.find_all('span', attrs={\"name\": \"nv\"}):\n",
    "                container.append(votes.text)\n",
    "        # Getting every 2nd element from the list because name:'nv' contains also gross $$$\n",
    "        info['votes_number'] = container[0]\n",
    "\n",
    "        ###### Scrapping director ######\n",
    "        print('Scrapping director')\n",
    "        r_director = requests.get(movie_url).text\n",
    "        s_director = BeautifulSoup(r_director, 'html.parser')\n",
    "\n",
    "        div = s_director.find('div', class_='credit_summary_item')\n",
    "        link = div.find_all('a')[0]\n",
    "        info['director'] = link.text\n",
    "\n",
    "        ###### Scrapping release date (pass country to main func) ######\n",
    "        print('Scrappint release date')\n",
    "        release_url = movie_url+'releaseinfo'\n",
    "        r_release = requests.get(release_url).text\n",
    "        s_release = BeautifulSoup(r_release, 'html.parser')\n",
    "\n",
    "        for tr in s_release.find_all('tr', class_='ipl-zebra-list__item release-date-item'):\n",
    "            if (tr.find_all('td', class_='release-date-item__country-name')[0].text == f'{release_country}\\n') & (not tr.find_all('td', class_='release-date-item__attributes')): #Second part makes sure that it wasn't a premiere on a festival\n",
    "                date_str = tr.find_all('td', class_='release-date-item__date')[0].text\n",
    "                release_date = parser.parse(date_str)\n",
    "                info['release_date'] = release_date\n",
    "\n",
    "        ###### Scrapping countries ######\n",
    "        print('Scrapping countries')\n",
    "        r_country = requests.get(movie_url).text\n",
    "        s_country = BeautifulSoup(r_country, 'html.parser')\n",
    "\n",
    "        countries = []\n",
    "        for div in s_country.find_all('div', id='titleDetails'):\n",
    "            for a in div.find_all('div', class_=\"txt-block\")[1].find_all('a'):\n",
    "                countries.append(a.text)\n",
    "        info['countries'] = countries\n",
    "\n",
    "        ###### Scrapping cast ######\n",
    "        print('Scrapping cast')\n",
    "        # Getting cast url\n",
    "        cast_url = movie_url+'fullcredits'\n",
    "        r_cast = requests.get(cast_url).text\n",
    "        s_cast = BeautifulSoup(r_cast, 'html.parser')\n",
    "        # Scrapping cast from the web\n",
    "        cast_list = s_cast.find_all('table', class_='cast_list')[0]\n",
    "        tr = cast_list.find_all('tr')\n",
    "        cast = []\n",
    "        for i in tr:\n",
    "            for td in i.find_all('td'):\n",
    "                if td.a:\n",
    "                    if 'name' in td.a.get('href'):\n",
    "                        cast.append(td.a.text)\n",
    "        # Deleting every 2nd item from  the list since it's a NaN\n",
    "        del cast[::2]\n",
    "        # Subtructing whitespace from the begining and break point from the end of the actors name\n",
    "        cast = [re.sub(r'(^ )|($\\n)', '', x) for x in cast]\n",
    "        info['cast'] = cast\n",
    "\n",
    "        ###### Scrapping plot ######\n",
    "        print('Scrapping plot')\n",
    "        # 2. Link that leads to synopsis\n",
    "        plot_url = movie_url+'plotsummary?ref_=tt_stry_pl#synopsis'\n",
    "        # 3. Create working synopsis link and get request model\n",
    "        r_synopsis = requests.get(plot_url).text\n",
    "        # 4. Use soup to parse synopsis html object\n",
    "        s_synopsis = BeautifulSoup(r_synopsis, 'html.parser')\n",
    "        # 5. Extract synopsis text\n",
    "        synopsis = s_synopsis.find('ul', id='plot-synopsis-content').li.text\n",
    "        info['plot'] = synopsis\n",
    "\n",
    "        ###### Scrapping original title. ######\n",
    "        print('Scrapping original title')\n",
    "        #If basic title = original it creates error. Get basic title.\n",
    "        try:\n",
    "            r_title = requests.get(movie_url).text\n",
    "            s_title = BeautifulSoup(r_title, 'html.parser')\n",
    "            original_title = s_title.find_all('div', class_='originalTitle')[0].text\n",
    "            original_title = re.sub(' \\(original title\\)', '', original_title)\n",
    "        except:\n",
    "            original_title = s_title.find_all('div', class_='title_wrapper')\n",
    "            original_title = re.search(r'[^\\xa0]*',original_title[0].h1.text).group()\n",
    "        info['original_title'] = original_title\n",
    "\n",
    "\n",
    "        data[h3.a.text] = info\n",
    "        clear_output()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to movies_951_1000.csv\n"
     ]
    }
   ],
   "source": [
    "# Scrapping movies in baches of 50 per imdb page i.e. (1,50,50) or (51,100,50)\n",
    "for i in range(1, 50, 50):\n",
    "    page = (i, i+49)\n",
    "    data = scrap_movies(page, i)\n",
    "    \n",
    "    movies = pd.DataFrame.from_dict(data.values())\n",
    "    movies['title'] = data.keys()\n",
    "    cols = ['title', 'original_title', 'year', 'release_date', 'rating', 'votes_number', 'runtime', 'certificate', 'countries', 'genres', 'plot', 'director', 'cast', 'title_id']\n",
    "    movies = movies[cols]\n",
    "    \n",
    "    path = r'C:\\Users\\Piotr\\Desktop\\Kurs_Python\\Datasets\\Movies\\Movies_csv\\\\'\n",
    "    # Saving to csv\n",
    "    print(f'Saving to movies_{page[0]}_{page[1]}.csv')\n",
    "    movies.to_csv(f'{path}movies_{page[0]}_{page[1]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading .csv files in a loop and concatenating them to movie data frame\n",
    "df_list = []\n",
    "path = r'C:\\Users\\Piotr\\Desktop\\Kurs_Python\\Datasets\\Movies\\Movies_csv\\\\'\n",
    "for i in range(1, 1000, 50):\n",
    "    page = (i, i+49)\n",
    "    df_list.append(pd.read_csv(f'{path}movies_{page[0]}_{page[1]}.csv'))\n",
    "imdb_movies = pd.concat(df_list, ignore_index=True)\n",
    "imdb_movies.to_csv('imdb_movies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
