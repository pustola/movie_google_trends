{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLEGEND:\\nr_ = request\\ns_ = soup\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "LEGEND:\n",
    "r_ = request\n",
    "s_ = soup\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function scraps synopsis and original title\n",
    "def scrap_synopsis(title_id):\n",
    "    '''\n",
    "    returns elements: synopsis, original_title\n",
    "    '''\n",
    "    \n",
    "    # 1. Create working link to a movie. Combingin constant imdb url with title ID : '/title/ID/'\n",
    "    movie_url = 'https://www.imdb.com'+title_id\n",
    "    # 2. Link that leads to synopsis\n",
    "    plot_url = 'plotsummary?ref_=tt_stry_pl#synopsis'\n",
    "    # 3. Create working synopsis link and get request model\n",
    "    r_synopsis = requests.get(movie_url + plot_url).text\n",
    "    # 4. Use soup to parse synopsis html object\n",
    "    s_synopsis = BeautifulSoup(r_synopsis, 'html.parser')\n",
    "    # 5. Extract synopsis text\n",
    "    synopsis = s_synopsis.find('ul', id='plot-synopsis-content').li.text\n",
    "    \n",
    "    # Scrapping original title. If basic title = original it creates error. Get basic title.\n",
    "    try:\n",
    "        r_title = requests.get(movie_url).text\n",
    "        s_title = BeautifulSoup(r_title, 'html.parser')\n",
    "        original_title = s_title.find_all('div', class_='originalTitle')[0].text\n",
    "        original_title = re.sub(' \\(original title\\)', '', original_title)\n",
    "    except:\n",
    "        original_title = s_title.find_all('div', class_='title_wrapper')\n",
    "        original_title = re.search(r'[^\\xa0]*',original_title[0].h1.text).group()\n",
    "    \n",
    "    return synopsis, original_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_country(title_id):\n",
    "    '''\n",
    "    returns list: countries if multiple or a one element list if it was one country production\n",
    "    '''\n",
    "    # 1. Create working link to a movie. Combingin constant imdb url with title ID : '/title/ID/'\n",
    "    movie_url = 'https://www.imdb.com'+title_id\n",
    "    # 3. Create working  link and get request model\n",
    "    r_country = requests.get(movie_url).text\n",
    "    # 4. Use soup to parse synopsis html object\n",
    "    s_country = BeautifulSoup(r_country, 'html.parser')\n",
    "    \n",
    "    countries = []\n",
    "    for div in s_country.find_all('div', id='titleDetails'):\n",
    "        for a in div.find_all('div', class_=\"txt-block\")[1].find_all('a'):\n",
    "            countries.append(a.text)\n",
    "    return countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_release_date(title_id, country='USA'):\n",
    "    '''\n",
    "    returns element: release_date\n",
    "    '''\n",
    "    release_date = None\n",
    "    release_url = 'https://www.imdb.com'+title_id+'releaseinfo'\n",
    "    r_release = requests.get(release_url).text\n",
    "    s_release = BeautifulSoup(r_release, 'html.parser')\n",
    "\n",
    "    for tr in s_release.find_all('tr', class_='ipl-zebra-list__item release-date-item'):\n",
    "        if (tr.find_all('td', class_='release-date-item__country-name')[0].text == f'{country}\\n') & (not tr.find_all('td', class_='release-date-item__attributes')): #Second part makes sure that it wasn't a premiere on a festival\n",
    "            date_str = tr.find_all('td', class_='release-date-item__date')[0].text\n",
    "            release_date = parser.parse(date_str)\n",
    "    return release_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basics():\n",
    "    '''\n",
    "    returns lists: title_ids, titles, years, ratings, votes\n",
    "    '''\n",
    "    title_ids = []\n",
    "    titles = []\n",
    "    years = []\n",
    "    ratings = []\n",
    "    votes = []\n",
    "\n",
    "    for page_counter in range(0,251,50): # Looping over next pages in top 250\n",
    "        print(f'Page: {page_counter}')\n",
    "        # 1. Get request model of top 250 imdb movies and convert it to str with .txt\n",
    "        r_top250 = requests.get(f'https://www.imdb.com/search/title/?groups=top_250&view=simple&sort=year,desc&start={page_counter}').text\n",
    "        # 2. Use soup to parse html object\n",
    "        s_top250 = BeautifulSoup(r_top250, 'html.parser')\n",
    "\n",
    "        # Scrapping title ID and title\n",
    "        for div in s_top250.find_all('div', class_='col-title'):\n",
    "            for link in div.find_all('a'):\n",
    "                title_ids.append(link.get('href'))\n",
    "                titles.append(link.text)\n",
    "\n",
    "        # Scrapping year    \n",
    "        for span in s_top250.find_all('span', class_='lister-item-year text-muted unbold'):\n",
    "            year_str = span.text\n",
    "            year = re.search('\\d\\d\\d\\d',year_str).group()\n",
    "            years.append(int(year))\n",
    "\n",
    "        # Scrapping rating and votes\n",
    "        for div in s_top250.find_all('div', class_='col-imdb-rating'):\n",
    "            for strong in div.find_all('strong'):\n",
    "                s = re.split(' ', strong.get('title'))\n",
    "                ratings.append(s[0])\n",
    "                votes.append(int(s[3].replace(',',''))) # Converting comma separated string to int ('115,334' = 115334)\n",
    "        clear_output()\n",
    "        \n",
    "    return title_ids, titles, years, ratings, votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "title_ids\n",
    "titles\n",
    "years\n",
    "ratings\n",
    "votes\n",
    "'''\n",
    "title_ids, titles, years, ratings, votes = basics()\n",
    "\n",
    "'''\n",
    "def scrap_synopsis / original title (title_ids)\n",
    "def scrap_country (title_ids)\n",
    "def scrap_release_date (title_ids)\n",
    "'''\n",
    "\n",
    "# Scrapping release dates into a list\n",
    "release_date = []\n",
    "for index, id_ in enumerate(title_ids):\n",
    "    print('Dates scrapping')\n",
    "    print(f'IDs left: {len(title_ids)-index}')\n",
    "    release_date.append(scrap_release_date(id_, country='USA'))\n",
    "    clear_output()\n",
    "\n",
    "# Scrapping countries into a list\n",
    "countries = []\n",
    "for index, id_ in enumerate(title_ids):\n",
    "    print('Countries scrapping')\n",
    "    print(f'IDs left: {len(title_ids)-index}')\n",
    "    countries.append(scrap_country(id_))\n",
    "    clear_output()\n",
    "    \n",
    "# Scrapping plots and original titles into a list\n",
    "plots = []\n",
    "original_titles = []\n",
    "for index, id_ in enumerate(title_ids):\n",
    "    print('Plots and original titles scrapping')\n",
    "    print(f'IDs left: {len(title_ids)-index}') # Printing counter and clearing output in the end\n",
    "    plot, title = scrap_synopsis(id_)\n",
    "    plots.append(plot)\n",
    "    original_titles.append(title)\n",
    "    clear_output()\n",
    "\n",
    "'''\n",
    "Create movies DF\n",
    "Create movies2004 DF\n",
    "'''\n",
    "movies = pd.DataFrame({'ID':title_ids, 'Title':titles, 'Original Title': original_titles, 'Year':years, 'Rating':ratings, 'Votes':votes, 'Country':countries, 'Plot':plots})\n",
    "\n",
    "# Replacing info about missing plot with None\n",
    "movies['Plot'] = movies['Plot'].apply(lambda x: None if 'looks like we don\\'t have' in x else x)\n",
    "\n",
    "movies2004 = movies[movies['Year'] >= 2004]\n",
    "\n",
    "'''\n",
    "movies to csv\n",
    "movies2004 to csv\n",
    "'''\n",
    "\n",
    "# Saving both dataframes to csv\n",
    "print('Saving to .csv')\n",
    "movies.to_csv('movies.csv', index=False)\n",
    "movies2004.to_csv('movies2004.csv', index=False)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
